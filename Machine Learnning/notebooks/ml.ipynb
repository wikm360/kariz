{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efbe244",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\" dir=\"ltr\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Kariz Course - Machine Learning Notebook</title>\n",
    "    <link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\">\n",
    "    <link href=\"https://fonts.googleapis.com/css2?family=Quicksand:wght@300;400;500;700&display=swap\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Quicksand', sans-serif;\n",
    "            background-color: #1a1a1a;\n",
    "            color: #d4d4d8;\n",
    "            line-height: 1.8;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 900px;\n",
    "            margin: 2rem auto;\n",
    "            padding: 2rem;\n",
    "            background: #2d2d2d;\n",
    "            border-radius: 12px;\n",
    "            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.4);\n",
    "        }\n",
    "        h1 {\n",
    "            font-size: 2.5rem;\n",
    "            color: #e5e7eb;\n",
    "            text-align: center;\n",
    "            margin-bottom: 1.5rem;\n",
    "            border-bottom: 2px solid #4b5563;\n",
    "            padding-bottom: 0.5rem;\n",
    "        }\n",
    "        h2 {\n",
    "            font-size: 1.75rem;\n",
    "            color: #60a5fa;\n",
    "            margin-top: 2rem;\n",
    "            margin-bottom: 1rem;\n",
    "        }\n",
    "        h3 {\n",
    "            font-size: 1.25rem;\n",
    "            color: #93c5fd;\n",
    "            margin-top: 1.5rem;\n",
    "            margin-bottom: 0.75rem;\n",
    "        }\n",
    "        p {\n",
    "            font-size: 1.1rem;\n",
    "            margin-bottom: 1rem;\n",
    "            text-align: justify;\n",
    "        }\n",
    "        .math-block {\n",
    "            background: #1f1f1f;\n",
    "            color: #d4d4d8;\n",
    "            padding: 1rem;\n",
    "            border-radius: 8px;\n",
    "            font-family: 'Courier New', Courier, monospace;\n",
    "            direction: ltr;\n",
    "            text-align: left;\n",
    "            margin: 1rem 0;\n",
    "            overflow-x: auto;\n",
    "            border-left: 4px solid #60a5fa;\n",
    "        }\n",
    "        .code-block {\n",
    "            background: #1f1f1f;\n",
    "            color: #d4d4d8;\n",
    "            padding: 1rem;\n",
    "            border-radius: 8px;\n",
    "            font-family: 'Courier New', Courier, monospace;\n",
    "            direction: ltr;\n",
    "            text-align: left;\n",
    "            margin: 1rem 0;\n",
    "            overflow-x: auto;\n",
    "            border-left: 4px solid #60a5fa;\n",
    "        }\n",
    "        .highlight {\n",
    "            background: #4b5563;\n",
    "            padding: 0.5rem;\n",
    "            border-radius: 6px;\n",
    "            display: inline-block;\n",
    "            margin: 0.5rem 0;\n",
    "            color: #e5e7eb;\n",
    "        }\n",
    "        .sidebar {\n",
    "            background: #262626;\n",
    "            padding: 1rem;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 2rem;\n",
    "            border: 1px solid #4b5563;\n",
    "        }\n",
    "        .sidebar ul {\n",
    "            list-style-type: disc;\n",
    "            padding-left: 1.5rem;\n",
    "        }\n",
    "        .sidebar li a {\n",
    "            color: #93c5fd;\n",
    "            transition: color 0.3s;\n",
    "        }\n",
    "        .sidebar li a:hover {\n",
    "            color: #60a5fa;\n",
    "        }\n",
    "        .button {\n",
    "            display: inline-block;\n",
    "            padding: 0.75rem 1.5rem;\n",
    "            background: #60a5fa;\n",
    "            color: #1a1a1a;\n",
    "            border-radius: 6px;\n",
    "            text-decoration: none;\n",
    "            transition: background 0.3s, color 0.3s;\n",
    "            margin-top: 1rem;\n",
    "        }\n",
    "        .button:hover {\n",
    "            background: #93c5fd;\n",
    "            color: #1a1a1a;\n",
    "        }\n",
    "        .image-placeholder {\n",
    "            background: #4b5563;\n",
    "            padding: 1rem;\n",
    "            border-radius: 8px;\n",
    "            margin: 1rem 0;\n",
    "            text-align: center;\n",
    "            color: #e5e7eb;\n",
    "            font-style: italic;\n",
    "        }\n",
    "        .created-by {\n",
    "            font-family: 'Vazir', sans-serif;\n",
    "            background: #262626;\n",
    "            color: #d4d4d8;\n",
    "            text-align: center;\n",
    "            padding: 0.75rem;\n",
    "            border-radius: 8px;\n",
    "            margin-top: 1.5rem;\n",
    "            font-size: 1rem;\n",
    "            border: 1px solid #4b5563;\n",
    "            transition: background 0.3s, color 0.3s;\n",
    "        }\n",
    "        .created-by a {\n",
    "            color: #60a5fa;\n",
    "            text-decoration: none;\n",
    "            font-weight: 700;\n",
    "        }\n",
    "        .created-by a:hover {\n",
    "            color: #93c5fd;\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "        .created-by span {\n",
    "            color: #f87171; /* Red heart */\n",
    "        }\n",
    "        @media (max-width: 768px) {\n",
    "            .created-by {\n",
    "                font-size: 0.9rem;\n",
    "                padding: 0.5rem;\n",
    "            }\n",
    "        }\n",
    "        @media (max-width: 768px) {\n",
    "            .container {\n",
    "                margin: 1rem;\n",
    "                padding: 1rem;\n",
    "            }\n",
    "            h1 {\n",
    "                font-size: 2rem;\n",
    "            }\n",
    "            h2 {\n",
    "                font-size: 1.5rem;\n",
    "            }\n",
    "            h3 {\n",
    "                font-size: 1.2rem;\n",
    "            }\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Kariz Course - Machine Learning Notebook</h1>\n",
    "        \n",
    "<div class=\"sidebar\">\n",
    "  <h2>Table of Contents</h2>\n",
    "  <ul>\n",
    "      <li><a href=\"#section1\">What is Machine Learning?</a></li>\n",
    "      <li><a href=\"#section2\">Supervised Learning</a></li>\n",
    "      <li><a href=\"#section3\">Regression Models</a></li>\n",
    "      <li><a href=\"#section4\">Training Process with Gradient Descent</a></li>\n",
    "      <li><a href=\"#section5\">Polynomial Regression</a></li>\n",
    "      <li><a href=\"#section6\">Introduction to Overfitting and Underfitting</a></li>\n",
    "      <li><a href=\"#section7\">Regularization Techniques</a></li>\n",
    "      <li><a href=\"#section8\">Introduction to Classification</a></li>\n",
    "      <li><a href=\"#section9\">Logistic Regression</a></li>\n",
    "      <li><a href=\"#section10\">Support Vector Machines (SVM)</a></li>\n",
    "      <li><a href=\"#section11\">Decision Trees</a></li>\n",
    "      <li><a href=\"#section12\">Random Forest</a></li>\n",
    "      <li><a href=\"#section13\">Introduction to Model Evaluation</a></li>\n",
    "      <li><a href=\"#section14\">Introduction to Unsupervised Learning</a></li>\n",
    "      <li><a href=\"#section15\">K-means</a></li>\n",
    "      <li><a href=\"#section16\">Dimensionality Reduction</a></li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"created-by\">\n",
    "    Created by <a href=\"https://wikm.ir\" target=\"_blank\">wikm.ir</a> with <span>❤️</span>\n",
    "</div>\n",
    "\n",
    "<section id=\"section1\">\n",
    "  <h2>What is Machine Learning?</h2>\n",
    "  <p>Machine Learning (ML) is a field that enables computers to learn without being explicitly programmed. It allows systems to improve their performance on a task through experience, as opposed to traditional direct programming.</p>\n",
    "  \n",
    "  <h3>Definition by Tom Mitchell (1998)</h3>\n",
    "  <p>\"A computer program is said to learn from experience <span class=\"highlight\">E</span> with respect to some task <span class=\"highlight\">T</span> and performance measure <span class=\"highlight\">P</span>, if its performance at task <span class=\"highlight\">T</span>, as measured by <span class=\"highlight\">P</span>, improves with experience <span class=\"highlight\">E</span>.\"</p>\n",
    "  <img src=\"./assets/Picture1.jpg\" alt=\"Tom Mitchell's Definition\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>Example: Checkers Game</h3>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Task (T)</strong>: Playing the game of checkers.</li>\n",
    "          <li><strong>Experience (E)</strong>: Playing thousands of games against itself.</li>\n",
    "          <li><strong>Performance Measure (P)</strong>: Number of wins against new opponents.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <img src=\"./assets/Picture3.jpg\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>Example: Spam Detection</h3>\n",
    "  <p>Suppose your email program allows you to mark emails as spam, and it learns to filter spam better based on your actions.</p>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Task (T)</strong>: Classifying emails as spam or not spam.</li>\n",
    "          <li><strong>Experience (E)</strong>: Observing which emails you mark as spam.</li>\n",
    "          <li><strong>Performance Measure (P)</strong>: Number of emails correctly classified.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    " <img src=\"./assets/Picture4.jpg\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>How Does a Machine Know It Has Improved?</h3>\n",
    "  <p>Machine learning improves performance through different approaches:</p>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Supervised Learning</strong>: The machine is given correct answers for a set of inputs, hoping it generalizes to new inputs (e.g., spam detection).</li>\n",
    "          <li><strong>Reinforcement Learning</strong>: The machine receives feedback on how correct its outputs are (e.g., a score) and learns to optimize its answers.</li>\n",
    "          <li><strong>Unsupervised Learning</strong>: No correct answers are provided; the machine finds patterns or groups similar data (e.g., clustering similar items).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  \n",
    "  <h3>Data in Machine Learning</h3>\n",
    "  <p>Data in machine learning can take various forms, such as images, audio, text, signals, or numerical data. These datasets often come with desired outputs, known as <span class=\"highlight\">Ground Truth</span> or labels. For example, a dataset might include images labeled as \"banana\" or \"apple\" to train a model to distinguish between them.</p>\n",
    "  <img src=\"./assets/Picture5.jpg\" class=\"image-placeholder\">\n",
    "\n",
    "<h3>Features and Labels</h3>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li><strong>Features</strong>: Input variables (e.g., house size, number of rooms, distance to city center).</li>\n",
    "        <li><strong>Labels</strong>: Target outputs (e.g., house price, \"positive\" or \"negative\" for disease detection).</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<img src=\"./assets/Feature-and-a-Labels-in-Machine-Learning.webp\" class=\"image-placeholder\">\n",
    "\n",
    "</section>\n",
    "\n",
    "<section id=\"section2\">\n",
    "  <h2>Supervised Learning</h2>\n",
    "  <p>Supervised learning involves training a model on a dataset where each input is paired with the correct output (label). The goal is to find a mapping function that can predict outputs for new inputs.</p>\n",
    "  \n",
    "  <h3>Real-World Inspiration</h3>\n",
    "  <p>Machine learning brings real-world learning to computers! Just as humans learn from teachers (e.g., parents teaching the names of animals or fruits), supervised learning provides data and correct answers to the machine. For example, a teacher shows images of dogs and labels them as \"dog\" to teach children to recognize dogs in various forms.</p>\n",
    "  <img src=\"./assets/Picture6.jpg\" class=\"image-placeholder\">\n",
    "\n",
    "  <h3>Goal</h3>\n",
    "  <p>The goal is to approximate a mapping function from inputs to outputs. Examples include:</p>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Spam Detection</strong>: Mapping emails to {spam, not spam}.</li>\n",
    "          <li><strong>Handwritten Digit Recognition</strong>: Mapping pixel images to digits {0, 1, ..., 9}.</li>\n",
    "          <li><strong>Cancer Detection</strong>: Mapping medical data to {malignant, benign}.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <img src=\"./assets/Picture8.jpg\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>Training Set Examples</h3>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Spam Detection</strong>: Input: Email, Output: Spam or Not Spam.</li>\n",
    "          <li><strong>Handwritten Digit Recognition</strong>: Input: Image of a digit, Output: A digit (0-9).</li>\n",
    "          <li><strong>House Pricing</strong>: Input: House size (in square feet), Output: Estimated price.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <img src=\"./assets/Picture7.jpg\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>Regression vs. Classification</h3>\n",
    "  <p>\n",
    "      <ul>\n",
    "        <li><strong>Regression</strong>: Predicting a continuous output variable. For example, predicting house prices based on features like size and location, where the output is a real number.</li>\n",
    "        <li><strong>Classification</strong>: Predicting a discrete or categorical output. For example, classifying an email as spam or not spam, where the output is a category (e.g., 0 or 1).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <img src=\"./assets/Regression_vs_Classification.avif\" class=\"image-placeholder\">\n",
    "</section>\n",
    "  \n",
    "<section id=\"section3\">\n",
    "  <h2>Regression Models</h2>\n",
    "  <p>Regression models aim to predict continuous outputs by modeling the relationship between input features and the target variable. Below are the key regression models with their mathematical formulations and algorithms.</p>\n",
    "\n",
    "  <h3>Linear Regression</h3>\n",
    "  <p>Linear regression assumes a linear relationship between the input features and the output. The model is represented as:</p>\n",
    "  <div class=\"math-block\">\n",
    "      y = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ\n",
    "  </div>\n",
    "  <p>where w₀ is the intercept (bias), w₁, ..., wₙ are the weights, and x₁, ..., xₙ are the feature values. The goal is to find the weights w that minimize the error between predicted and actual values (with Gradient Descent) or solve the Normal Equetion.</p>\n",
    "  <img src=\"./assets/regression-converge.gif\" class=\"image-placeholder\">\n",
    "\n",
    "  <h3>Cost Functions</h3>\n",
    "  <p>A cost function (or loss function) quantifies the error between a model's predictions and the true labels, guiding the learning process. The goal of supervised learning is to minimize this cost function to improve predictions.</p>\n",
    "  <img src=\"./assets/cost.jpg\" class=\"image-placeholder\">\n",
    "\n",
    "  <h3>Cost Functions for Regression</h3>\n",
    "  <p>For regression tasks, the cost function measures the difference between predicted and actual continuous values. Common cost functions include:</p>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Mean Squared Error (MSE)</strong>: Measures the average squared difference between predictions and actual values. It is sensitive to outliers due to the squaring operation.</li>\n",
    "          <div class=\"math-block\">\n",
    "              J(w) = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ)²\n",
    "          </div>\n",
    "          <p>where h(xᵢ) is the predicted value, yᵢ is the true value, and m is the number of samples.</p>\n",
    "          <li><strong>Mean Absolute Error (MAE)</strong>: Measures the average absolute difference, less sensitive to outliers.</li>\n",
    "          <div class=\"math-block\">\n",
    "              J(w) = (1/m) Σᵢ₌₁ᵐ |h(xᵢ) - yᵢ|\n",
    "          </div>\n",
    "      </ul>\n",
    "  </p>\n",
    " <img src=\"./assets/rmse_vs_mae.png\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section4\">\n",
    "  <h2>Training Process with Gradient Descent</h2>\n",
    "<p>For large datasets or when the normal equation is impractical, gradient descent is used to iteratively optimize the weights. The process involves initializing the weights, computing the gradient of the cost function, and updating the weights until convergence to find the parameters that best fit the data.</p>\n",
    "\n",
    "<div class=\"math-block\">\n",
    "    w := w - η ∇J(w)\n",
    "</div>\n",
    "\n",
    "<p><strong>Parameters</strong>:\n",
    "    <ul>\n",
    "        <li>w = [w₀, w₁, ..., wₙ]: The weight vector to be optimized.</li>\n",
    "        <li>η (eta): The learning rate, controlling the step size of weight updates (e.g., 0.01).</li>\n",
    "        <li>∇J(w): The gradient of the cost function with respect to w.</li>\n",
    "        <li>Epochs: The number of iterations over the entire dataset.</li>\n",
    "        <li>Convergence criteria: A threshold for stopping (e.g., when J(w) changes less than a small ε).</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p><strong>Gradient of the Cost Function</strong>:</p>\n",
    "<div class=\"math-block\">\n",
    "    ∇J(w) = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ) xᵢ\n",
    "</div>\n",
    "<p>For each weight wⱼ (including w₀, where x₀ = 1):</p>\n",
    "<div class=\"math-block\">\n",
    "    ∂J(w)/∂wⱼ = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ) x_{i,j}\n",
    "</div>\n",
    "<p><strong>Training Algorithm (Gradient Descent)</strong>:</p>\n",
    "<p>\n",
    "    <ol>\n",
    "        <li><strong>Initialize Weights</strong>: Set initial values for w = [w₀, w₁, ..., wₙ], typically to small random values (e.g., from a normal distribution N(0, 0.01)) or zeros. Random initialization helps avoid symmetry in optimization.</li>\n",
    "        <li><strong>Compute Predictions</strong>: For each sample xᵢ, compute h(xᵢ) = w₀ + w₁x₁ + ... + wₙxₙ.</li>\n",
    "        <li><strong>Calculate Gradient</strong>: Compute the gradient ∇J(w) using the entire dataset (batch gradient descent).</li>\n",
    "        <li><strong>Update Weights</strong>: Update each weight using:</li>\n",
    "        <div class=\"math-block\">\n",
    "            wⱼ := wⱼ - η (∂J(w)/∂wⱼ) = wⱼ - η (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ) x_{i,j}\n",
    "        </div>\n",
    "        <li><strong>Repeat</strong>: Iterate steps 2-4 for a fixed number of epochs or until convergence (e.g., when |J(wₜ) - J(wₜ₋₁)| < ε, where ε is a small threshold like 10⁻⁶).</li>\n",
    "        <li><strong>Output</strong>: The final weights w* represent the parameters that best fit the data, minimizing J(w).</li>\n",
    "    </ol>\n",
    "</p>\n",
    "<p><strong>Convergence</strong>: As gradient descent iterates, the weights gradually adjust to reduce J(w), converging to a local or global minimum (since MSE is convex, it has a single global minimum). The final weights w* define the line (or hyperplane) that best fits the training data.</p>\n",
    "<img src=\"./assets/Gradient_descent.gif\" class=\"image-placeholder\">\n",
    "<p>https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html</p>\n",
    "\n",
    "<h2>Practical Considerations</h2>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li><strong>Learning Rate (η)</strong>: A small η ensures stability but slows convergence; a large η may cause overshooting.</li>\n",
    "        <li><strong>Feature Scaling</strong>: Features should be standardized (e.g., mean=0, std=1) to ensure consistent gradient scales.</li>\n",
    "        <li><strong>Convergence Check</strong>: Monitor J(w) or use early stopping to avoid unnecessary iterations.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<img src=\"./assets/learning-rate.png\" class=\"image-placeholder\">\n",
    "\n",
    "  <h3>Types of Gradient Descent</h3>\n",
    "\n",
    "  <h3>1.Batch Gradient Descent</h3>\n",
    "  <p>Batch Gradient Descent is an optimization algorithm that computes the gradient of the cost function with respect to the model parameters using the entire training dataset in each iteration. It updates the parameters (weights) by taking a step in the direction of the negative gradient, aiming to minimize the cost function:</p>\n",
    "  <div class=\"math-block\">\n",
    "      w := w - η ∇J(w)\n",
    "  </div>\n",
    "  <p>where η is the learning rate, and ∇J(w) is the gradient of the cost function with respect to w. The gradient is computed as:</p>\n",
    "  <div class=\"math-block\">\n",
    "      ∇J(w) = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ) xᵢ\n",
    "  </div>\n",
    "  <p><strong>Algorithm</strong>:</p>\n",
    "  <p>\n",
    "      <ol>\n",
    "          <li>Initialize weights w randomly or with zeros.</li>\n",
    "          <li>Compute the gradient ∇J(w) using the entire dataset.</li>\n",
    "          <li>Update weights: w := w - η ∇J(w).</li>\n",
    "          <li>Repeat until convergence (e.g., when J(w) stabilizes or a maximum number of iterations is reached).</li>\n",
    "      </ol>\n",
    "  </p>\n",
    "  <p>Gradient Descent uses the entire dataset, making it computationally expensive for large datasets.</p>\n",
    "\n",
    "  <p>Analogy: Imagine you’re teaching a young child to recognize animals by showing them all 1,000 animal pictures at once and saying, “Learn everything together, now!” The child tries to memorize the patterns across all pictures before making any updates to their understanding. This is thorough but slow, as it requires processing the entire set of pictures in one go.</p>\n",
    "  \n",
    "  <h3>2. Stochastic Gradient Descent (SGD)</h3>\n",
    "  <p>SGD updates weights using the gradient of a single sample at each iteration, reducing computational cost:</p>\n",
    "  <div class=\"math-block\">\n",
    "      w := w - η (h(xᵢ) - yᵢ) xᵢ\n",
    "  </div>\n",
    "  <p><strong>Algorithm</strong>:</p>\n",
    "  <p>\n",
    "      <ol>\n",
    "          <li>Initialize weights w.</li>\n",
    "          <li>For each sample (xᵢ, yᵢ) in random order, compute the gradient and update w.</li>\n",
    "          <li>Repeat for multiple epochs until convergence.</li>\n",
    "      </ol>\n",
    "  </p>\n",
    "  <p>SGD is noisy but faster, suitable for large datasets.</p>\n",
    "\n",
    "  <p>Analogy: Instead of overwhelming the child with 1,000 pictures, you show them one picture at a time and say, “Learn this one, then move to the next.” The child updates their understanding after each picture, making quick but sometimes inconsistent progress because they’re learning from one example at a time.</p>\n",
    "  \n",
    "  <h3>3. Mini-Batch Gradient Descent</h3>\n",
    "  <p>Mini-Batch Gradient Descent balances Gradient Descent and SGD by using a small batch of samples (e.g., 32 or 64) to compute the gradient:</p>\n",
    "  <div class=\"math-block\">\n",
    "      w := w - η (1/b) Σᵢ∈batch (h(xᵢ) - yᵢ) xᵢ\n",
    "  </div>\n",
    "  <p>where b is the batch size. This method is widely used in practice due to its efficiency and stability.</p>\n",
    "  \n",
    "  <h3>4. Advanced Optimization Algorithms</h3>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>Momentum</strong>: Accelerates gradient descent by adding a fraction of the previous update:</li>\n",
    "          <li><strong>Adam (Adaptive Moment Estimation)</strong>: Combines adaptive learning rates with momentum, using moving averages of gradients and squared gradients:</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "\n",
    "  <p>Analogy: You show the child a small stack of 10 pictures at a time (instead of 1 or 1,000) and say, “Learn these together, then move to the next stack.” The child learns from a manageable group, making steady progress without being overwhelmed or overly focused on a single picture.</p>\n",
    "\n",
    "  <img src=\"./assets/batch__stochastic__mini-batch_gradient_descent-Dec-22-2022-04-32-42-4986-AM.webp\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section5\">\n",
    "  <h3>2. Polynomial Regression</h3>\n",
    "  <p>Polynomial regression extends linear regression by fitting a polynomial function to the data, allowing for non-linear relationships. The model is expressed as:</p>\n",
    "  <div class=\"math-block\">\n",
    "      y = w₀ + w₁x + w₂x² + ... + wₖxᵏ\n",
    "  </div>\n",
    "  <p>where k is the degree of the polynomial. The features are transformed into polynomial terms (e.g., x, x², x³), and linear regression is applied to these transformed features.</p>\n",
    "  <p><strong>Algorithm</strong>: Similar to linear regression, the weights are optimized using OLS or gradient descent. However, higher-degree polynomials can lead to overfitting, requiring regularization (discussed later).</p>\n",
    "  <img src=\"./assets/poly.webp\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section6\">\n",
    "    <h2>Introduction to Overfitting and Underfitting</h2>\n",
    "    <p>In machine learning, a model’s performance is evaluated based on its ability to generalize to unseen data. Two common issues that affect generalization are <strong>overfitting</strong> and <strong>underfitting</strong>.</p>\n",
    "    <p><strong>Overfitting</strong>: Occurs when a model learns not only the underlying patterns in the training data but also its noise and outliers, resulting in excellent performance on the training set but poor performance on the test set. The model is too complex and fits the training data too closely.</p>\n",
    "    <p><strong>Underfitting</strong>: Occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and test sets. The model fails to learn the relationships between features and labels.</p>\n",
    "    <p>The goal is to achieve a balance, where the model is complex enough to capture patterns but simple enough to generalize well.</p>\n",
    "    <img src=\"./assets/Bias-and-Variance-in-Machine-Learning.webp\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section7\">\n",
    "  <h2>Regularization Techniques</h2>\n",
    "  <p>Regularization prevents overfitting by adding a penalty term to the cost function, discouraging overly complex models. It improves generalization to unseen data.</p>\n",
    "  \n",
    "  <h3>1. L2 Regularization (Ridge)</h3>\n",
    "  <p>L2 regularization adds the squared magnitude of weights to the cost function:</p>\n",
    "  <div class=\"math-block\">\n",
    "      J(w) = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ)² + λ Σᵢ₌₁ⁿ wᵢ²\n",
    "  </div>\n",
    "  <p>where λ is the regularization parameter. The penalty shrinks weights toward zero, reducing model complexity without eliminating features.</p>\n",
    "  <p><strong>Effect</strong>: Smooths the model, making it less sensitive to individual features.</p>\n",
    "  \n",
    "  <h3>2. L1 Regularization (Lasso)</h3>\n",
    "  <p>L1 regularization adds the absolute magnitude of weights:</p>\n",
    "  <div class=\"math-block\">\n",
    "      J(w) = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ)² + λ Σᵢ₌₁ⁿ |wᵢ|\n",
    "  </div>\n",
    "  <p>The L1 penalty can drive some weights to exactly zero, effectively performing feature selection.</p>\n",
    "  <p><strong>Effect</strong>: Produces sparse models, useful when only a few features are relevant.</p>\n",
    "  \n",
    "  <h3>3. Elastic Net Regularization</h3>\n",
    "  <p>Elastic Net combines L1 and L2 regularization:</p>\n",
    "  <div class=\"math-block\">\n",
    "      J(w) = (1/m) Σᵢ₌₁ᵐ (h(xᵢ) - yᵢ)² + λ₁ Σᵢ₌₁ⁿ |wᵢ| + λ₂ Σᵢ₌₁ⁿ wᵢ²\n",
    "  </div>\n",
    "  <p>It balances sparsity (L1) and smoothness (L2), controlled by λ₁ and λ₂.</p>\n",
    "  <p><strong>Algorithm</strong>: Optimized using coordinate descent or gradient-based methods.</p>\n",
    "  \n",
    "  <h3>4. Dropout (for Neural Networks)</h3>\n",
    "  <p>Dropout is a regularization technique for neural networks where a random fraction of neurons are ignored during each training iteration, preventing co-adaptation of neurons.</p>\n",
    "  <p><strong>Mechanism</strong>: During training, each neuron is dropped with probability p (e.g., 0.5). During inference, all neurons are used, but weights are scaled by (1-p) to maintain expected output.</p>\n",
    "  \n",
    "  <h3>5. Early Stopping</h3>\n",
    "  <p>Early stopping halts training when the performance on a validation set stops improving, preventing overfitting.</p>\n",
    "  <p><strong>Algorithm</strong>:</p>\n",
    "  <p>\n",
    "      <ol>\n",
    "          <li>Split data into training and validation sets.</li>\n",
    "          <li>Train the model and monitor validation loss.</li>\n",
    "          <li>Stop training if validation loss does not improve after a fixed number of epochs (patience).</li>\n",
    "      </ol>\n",
    "  </p>\n",
    "  <img src=\"./assets/early.png\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section8\">\n",
    "  <h2>Introduction to Classification</h2>\n",
    "  <p>Classification is a supervised learning task where the goal is to predict a discrete label or category for each input sample. Given a dataset {(x₁, y₁), ..., (xₘ, yₘ)}, where xᵢ is a feature vector and yᵢ ∈ {C₁, ..., Cₖ} is a categorical label, the model learns a function h(x) that maps inputs to one of K classes.</p>\n",
    "  <p>Classification problems can be binary (K=2, e.g., spam vs. not spam) or multi-class (K>2, e.g., classifying digits 0-9). The output is typically a class label or a probability distribution over classes.</p>\n",
    "  <img src=\"./assets/decisionn boundry.png\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section9\">\n",
    "  <h2>Logistic Regression</h2>\n",
    "  <p>Logistic Regression is a linear model for binary classification that predicts the probability of a sample belonging to a class. It uses the logistic (sigmoid) function to map a linear combination of features to a probability between 0 and 1:</p>\n",
    "  <div class=\"math-block\">\n",
    "      h(x) = σ(w₀ + w₁x₁ + ... + wₙxₙ) = 1 / (1 + e^-(w₀ + w₁x₁ + ... + wₙxₙ))\n",
    "  </div>\n",
    "  <p>where σ(z) is the sigmoid function, and w₀, ..., wₙ are the weights. The predicted class is 1 if h(x) ≥ 0.5, otherwise 0.</p>\n",
    "  <p><strong>Cost Function</strong>: Logistic Regression uses binary cross-entropy (log loss):</p>\n",
    "  <div class=\"math-block\">\n",
    "      J(w) = -(1/m) Σᵢ₌₁ᵐ [yᵢ log(h(xᵢ)) + (1 - yᵢ) log(1 - h(xᵢ))]\n",
    "  </div>\n",
    "  <p><strong>Algorithm</strong>: Weights are optimized using gradient descent or variants (e.g., Adam).</p>\n",
    "  <img src=\"./assets/sigmoid.png\" class=\"image-placeholder\">\n",
    "\n",
    "  <p><strong>Multi-Class Extension</strong>: For multi-class problems, Logistic Regression is extended to Softmax Regression, where the output is a probability distribution over K classes:</p>\n",
    "  <div class=\"math-block\">\n",
    "      h_k(x) = e^(w_k·x) / Σᵢ₌₁ᴷ e^(w_i·x)\n",
    "      J(w) = -(1/m) Σᵢ₌₁ᵐ Σᵏ₌₁ᴷ y_{i,k} log(h_k(xᵢ))\n",
    "  </div>\n",
    "</section>\n",
    "\n",
    "\n",
    "\n",
    "<section id=\"section10\">\n",
    "  <h2>Support Vector Machines (SVM)</h2>\n",
    "  <p>Support Vector Machines find the optimal hyperplane that maximizes the margin between classes in a classification problem. For linearly separable data, the hyperplane is defined as:</p>\n",
    "  <div class=\"math-block\">\n",
    "      w·x + b = 0\n",
    "  </div>\n",
    "  <p>where w is the weight vector, and b is the bias. The margin is the distance between the hyperplane and the nearest data points (support vectors).</p>\n",
    "  <p><strong>Optimization Objective</strong>: Maximize the margin, equivalent to minimizing:</p>\n",
    "  <div class=\"math-block\">\n",
    "      J(w) = (1/2) ||w||² subject to yᵢ(w·xᵢ + b) ≥ 1 for all i\n",
    "  </div>\n",
    "  <p><strong>Soft Margin (Non-Separable Data)</strong>: For non-linearly separable data, a soft margin allows some misclassifications by introducing slack variables ξᵢ:</p>\n",
    "  <div class=\"math-block\">\n",
    "      J(w, b, ξ) = (1/2) ||w||² + C Σᵢ₌₁ᵐ ξᵢ\n",
    "      subject to yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0\n",
    "  </div>\n",
    "  <p>where C controls the trade-off between margin maximization and classification error.</p>\n",
    "  <img src=\"./assets/support-vector-machine-1.png\" class=\"image-placeholder\">\n",
    "\n",
    "  <p><strong>Kernel Trick</strong>: For non-linear boundaries, SVM uses a kernel function (e.g., RBF kernel) to map data to a higher-dimensional space:</p>\n",
    "  <div class=\"math-block\">\n",
    "      K(xᵢ, xⱼ) = exp(-γ ||xᵢ - xⱼ||²)\n",
    "  </div>\n",
    "  <p><strong>Algorithm</strong>: The optimization is solved using quadratic programming or sequential minimal optimization (SMO).</p>\n",
    "  <img src=\"./assets/kernel.png\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section11\">\n",
    "  <h2>Decision Trees</h2>\n",
    "  <p>Decision Trees are a supervised learning model used for both classification and regression tasks. They work by recursively partitioning the feature space into regions based on feature values, creating a tree-like structure to make decisions. Each node in the tree represents a decision or condition, and each leaf represents a final output (class label for classification or value for regression).</p>\n",
    "  \n",
    "  <h3>Structure and Functionality</h3>\n",
    "  <p>A decision tree consists of:\n",
    "      <ul>\n",
    "          <li><strong>Root Node</strong>: The topmost node, representing the entire dataset and the first feature-based decision.</li>\n",
    "          <li><strong>Internal Nodes</strong>: Represent decisions based on feature values (e.g., \"Is age > 30?\"). Each node splits the data into two or more subsets.</li>\n",
    "          <li><strong>Branches</strong>: Connect nodes, representing the outcome of a decision (e.g., \"Yes\" or \"No\").</li>\n",
    "          <li><strong>Leaf Nodes</strong>: Terminal nodes that provide the final output (e.g., a class label like \"Positive\" or \"Negative\").</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>How It Works</strong>: To classify a new sample, start at the root node and follow the decision path based on the sample’s feature values until reaching a leaf node, which provides the predicted class. For example, in a medical diagnosis tree, the root might ask, \"Is fever present?\" If yes, the next node might check, \"Is cough present?\" leading to a leaf node predicting \"Flu\" or \"No Flu.\"</p>\n",
    "  <p><strong>Advantages</strong>:\n",
    "      <ul>\n",
    "          <li>Easy to understand and visualize.</li>\n",
    "          <li>Handles both numerical and categorical features.</li>\n",
    "          <li>No need for feature scaling.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Disadvantages</strong>:\n",
    "      <ul>\n",
    "          <li>Prone to overfitting, especially with deep trees.</li>\n",
    "          <li>Sensitive to small changes in data.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <img src=\"./assets/what-is-a-decision-tree.webp\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>Entropy</h3>\n",
    "  <p>Entropy measures the impurity or uncertainty in a dataset. It quantifies how mixed the classes are in a node. For a node with K classes, entropy is defined as:</p>\n",
    "  <div class=\"math-block\">\n",
    "      Entropy = -Σᵏ₌₁ᴷ p_k log₂(p_k)\n",
    "  </div>\n",
    "  <p>where:\n",
    "      <ul>\n",
    "          <li>p_k: The proportion of samples in class k (i.e., number of samples in class k divided by total samples in the node).</li>\n",
    "          <li>log₂: The base-2 logarithm, commonly used in information theory.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Interpretation</strong>:\n",
    "      <ul>\n",
    "          <li>Entropy = 0: The node is pure (all samples belong to one class).</li>\n",
    "          <li>Entropy is maximized (e.g., 1 for two classes): The node is perfectly mixed (equal number of samples in each class).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p>For example, if a node has 10 samples (6 in class \"Positive,\" 4 in class \"Negative\"), entropy is:\n",
    "      <ul>\n",
    "          <li>p_Positive = 6/10 = 0.6, p_Negative = 4/10 = 0.4</li>\n",
    "          <li>Entropy = -[0.6 log₂(0.6) + 0.4 log₂(0.4)] ≈ 0.971</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  \n",
    "  <h3>Information Gain (IG)</h3>\n",
    "  <p>Information Gain measures the reduction in entropy after splitting a node based on a feature. It helps select the best feature and threshold for splitting by maximizing the purity of child nodes. IG is defined as:</p>\n",
    "  <div class=\"math-block\">\n",
    "      IG = Entropy(parent) - Σᵢ₌₁ᵗ (nᵢ/n) Entropy(childᵢ)\n",
    "  </div>\n",
    "  <p>where:\n",
    "      <ul>\n",
    "          <li>Entropy(parent): Entropy of the parent node before splitting.</li>\n",
    "          <li>nᵢ: Number of samples in child node i.</li>\n",
    "          <li>n: Total number of samples in the parent node.</li>\n",
    "          <li>Entropy(childᵢ): Entropy of child node i after splitting.</li>\n",
    "          <li>t: Number of child nodes (e.g., 2 for a binary split).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p>The feature and threshold that maximize IG are chosen for the split.</p>\n",
    "  \n",
    "  <h3>Example: Calculating Information Gain</h3>\n",
    "  <p>Consider a dataset with 10 samples for binary classification (Positive/Negative) to predict if a patient has a disease based on two features: \"Fever\" (Yes/No) and \"Cough\" (Yes/No). The parent node has 6 Positive and 4 Negative samples. We evaluate splitting on \"Fever.\"</p>\n",
    "  <p><strong>Step 1: Entropy of Parent Node</strong>:\n",
    "      <ul>\n",
    "          <li>p_Positive = 6/10 = 0.6, p_Negative = 4/10 = 0.4</li>\n",
    "          <li>Entropy(parent) = -[0.6 log₂(0.6) + 0.4 log₂(0.4)]</li>\n",
    "          <li>log₂(0.6) ≈ -0.737, log₂(0.4) ≈ -1.322</li>\n",
    "          <li>Entropy(parent) = -[0.6 × (-0.737) + 0.4 × (-1.322)] = -[-0.4422 - 0.5288] ≈ 0.971</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Step 2: Split on Fever</strong>:\n",
    "      <ul>\n",
    "          <li><strong>Fever = Yes</strong>: 5 samples (4 Positive, 1 Negative)</li>\n",
    "          <li>p_Positive = 4/5 = 0.8, p_Negative = 1/5 = 0.2</li>\n",
    "          <li>Entropy(Yes) = -[0.8 log₂(0.8) + 0.2 log₂(0.2)]</li>\n",
    "          <li>log₂(0.8) ≈ -0.322, log₂(0.2) ≈ -2.322</li>\n",
    "          <li>Entropy(Yes) = -[0.8 × (-0.322) + 0.2 × (-2.322)] = -[-0.2576 - 0.4644] ≈ 0.722</li>\n",
    "          <li><strong>Fever = No</strong>: 5 samples (2 Positive, 3 Negative)</li>\n",
    "          <li>p_Positive = 2/5 = 0.4, p_Negative = 3/5 = 0.6</li>\n",
    "          <li>Entropy(No) = -[0.4 log₂(0.4) + 0.6 log₂(0.6)] ≈ 0.971</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Step 3: Weighted Entropy</strong>:\n",
    "      <ul>\n",
    "          <li>Weight of Yes node = 5/10 = 0.5, Weight of No node = 5/10 = 0.5</li>\n",
    "          <li>Weighted Entropy = 0.5 × 0.722 + 0.5 × 0.971 = 0.361 + 0.4855 = 0.8465</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Step 4: Information Gain</strong>:\n",
    "      <ul>\n",
    "          <li>IG = Entropy(parent) - Weighted Entropy = 0.971 - 0.8465 ≈ 0.1245</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Conclusion</strong>: The IG for splitting on \"Fever\" is 0.1245. Repeat this for other features (e.g., \"Cough\") and choose the feature with the highest IG for the split.</p>\n",
    "  \n",
    "  <h3>Gini Impurity</h3>\n",
    "  <p>Gini Impurity is an alternative measure of node impurity, often used instead of entropy due to its computational simplicity (no logarithms). It measures the probability of incorrectly classifying a randomly chosen sample:</p>\n",
    "  <div class=\"math-block\">\n",
    "      Gini = 1 - Σᵏ₌₁ᴷ p_k²\n",
    "  </div>\n",
    "  <p>where p_k is the proportion of samples in class k.</p>\n",
    "  <p><strong>Interpretation</strong>:\n",
    "      <ul>\n",
    "          <li>Gini = 0: The node is pure (all samples in one class).</li>\n",
    "          <li>Gini is maximized (e.g., 0.5 for two classes): The node is perfectly mixed.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p>For the earlier example (6 Positive, 4 Negative):\n",
    "      <ul>\n",
    "          <li>p_Positive = 0.6, p_Negative = 0.4</li>\n",
    "          <li>Gini = 1 - [0.6² + 0.4²] = 1 - [0.36 + 0.16] = 1 - 0.52 = 0.48</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Splitting</strong>: The feature and threshold that minimize the weighted Gini impurity of child nodes are chosen.</p>\n",
    "  \n",
    "  <h3>Decision Tree Algorithm</h3>\n",
    "  <p>\n",
    "      <ol>\n",
    "          <li>Start with the root node containing all samples.</li>\n",
    "          <li>For each feature and possible threshold, calculate IG or Gini impurity for the resulting split.</li>\n",
    "          <li>Select the feature and threshold that maximize IG or minimize Gini.</li>\n",
    "          <li>Split the data into child nodes based on the selected feature and threshold.</li>\n",
    "          <li>Repeat recursively for each child node until a stopping criterion is met (e.g., maximum depth, minimum samples per node, or pure nodes).</li>\n",
    "          <li>Assign the majority class to each leaf node.</li>\n",
    "      </ol>\n",
    "  </p>\n",
    "</section>\n",
    "\n",
    "<section id=\"section12\">\n",
    "  <h2>Random Forest</h2>\n",
    "  <p>Random Forest is an ensemble learning method that combines multiple decision trees to improve accuracy and robustness while reducing overfitting. It’s like a team of decision trees working together, where each tree gives its opinion, and the final prediction is made by majority vote (for classification) or averaging (for regression). The “randomness” in Random Forest makes it more generalizable than a single decision tree.</p>\n",
    "  \n",
    "  <h3>How Random Forest Works</h3>\n",
    "  <p>Imagine you’re trying to decide if a movie is good or bad. Instead of asking one friend (a single decision tree), you ask 100 friends, each with slightly different perspectives (different data or features). You then take a vote to make the final decision. Random Forest does this by:\n",
    "      <ul>\n",
    "          <li><strong>Bagging (Bootstrap Aggregating)</strong>: Each tree is trained on a random subset of the training data, sampled with replacement. This means some samples may appear multiple times in a tree’s dataset, while others may be left out.</li>\n",
    "          <li><strong>Feature Randomness</strong>: At each node of a tree, only a random subset of features is considered for splitting (e.g., if you have 10 features, a tree might only look at 3 randomly chosen ones).</li>\n",
    "          <li><strong>Voting/Averaging</strong>: For classification, the final prediction is the class that gets the most votes from all trees. For regression, it’s the average of all tree predictions.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Why It Works</strong>:\n",
    "      <ul>\n",
    "          <li>Individual decision trees often overfit, but combining many trees reduces this by averaging out errors.</li>\n",
    "          <li>Randomness in data and features ensures trees are diverse, making the model less sensitive to noise.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <img src=\"./assets/Ensemble-of-decision-trees.png\" class=\"image-placeholder\">\n",
    "  \n",
    "  <h3>Mathematical Objective</h3>\n",
    "  <p>For classification, the Random Forest prediction is the mode of individual tree predictions:</p>\n",
    "  <div class=\"math-block\">\n",
    "      h(x) = mode(h₁(x), h₂(x), ..., hₜ(x))\n",
    "  </div>\n",
    "  <p>where:\n",
    "      <ul>\n",
    "          <li>h₁(x), ..., hₜ(x): Predictions from T decision trees.</li>\n",
    "          <li>mode: The most frequent class predicted by the trees.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p>The expected error is reduced because the variance of the ensemble is lower than that of a single tree:</p>\n",
    "  <div class=\"math-block\">\n",
    "      Var(h(x)) ≤ (1/T) Σᵢ₌₁ᵗ Var(hᵢ(x))\n",
    "  </div>\n",
    "  <p>provided the trees are not perfectly correlated (randomness helps achieve this).</p>\n",
    "  \n",
    "  <h3>Random Forest Algorithm</h3>\n",
    "  <p>\n",
    "      <ol>\n",
    "          <li><strong>Generate Bootstrap Samples</strong>: Create T random subsets of the training data by sampling with replacement (each subset is roughly the same size as the original dataset).</li>\n",
    "          <li><strong>Build Decision Trees</strong>: For each subset, construct a decision tree:\n",
    "              <ul>\n",
    "                  <li>At each node, randomly select a subset of features (e.g., √n features, where n is the total number of features).</li>\n",
    "                  <li>Choose the best split among these features using IG or Gini.</li>\n",
    "                  <li>Grow the tree to a specified depth or until a stopping criterion (e.g., minimum samples per leaf).</li>\n",
    "              </ul>\n",
    "          </li>\n",
    "          <li><strong>Combine Predictions</strong>: For a new sample, collect predictions from all T trees and output the majority vote (classification) or average (regression).</li>\n",
    "      </ol>\n",
    "  </p>\n",
    "  <p><strong>Hyperparameters</strong>:\n",
    "      <ul>\n",
    "          <li><strong>Number of Trees (T)</strong>: More trees improve stability but increase computation (e.g., T=100).</li>\n",
    "          <li><strong>Feature Subset Size</strong>: Typically √n or log₂(n) features per split.</li>\n",
    "          <li><strong>Max Depth</strong>: Limits tree depth to prevent overfitting.</li>\n",
    "          <li><strong>Minimum Samples per Leaf</strong>: Ensures leaves have enough samples to avoid overly specific splits.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Advantages</strong>:\n",
    "      <ul>\n",
    "          <li>Reduces overfitting compared to a single decision tree.</li>\n",
    "          <li>Handles high-dimensional data and mixed feature types.</li>\n",
    "          <li>Provides feature importance scores based on how much each feature reduces impurity.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Disadvantages</strong>:\n",
    "      <ul>\n",
    "          <li>More computationally intensive than a single tree.</li>\n",
    "          <li>Less interpretable than a single decision tree.</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "</section>\n",
    "\n",
    "  <section id=\"section13\">\n",
    "  <h2>Introduction to Model Evaluation</h2>\n",
    "  <p>Model evaluation is a critical step in machine learning to assess how well a model performs on unseen data. For classification tasks, metrics like accuracy, precision, recall, and F1-score are derived from the confusion matrix, which provides a detailed breakdown of predictions. For regression, metrics like Mean Squared Error (MSE) are used. This section focuses on classification metrics, with an in-depth explanation of the confusion matrix.</p>\n",
    "  <img src=\"./assets/eval.png\" class=\"image-placeholder\">\n",
    "\n",
    "  <h2>Confusion Matrix</h2>\n",
    "  <p>The <strong>confusion matrix</strong> is a table that summarizes the performance of a classification model by comparing predicted labels to actual labels. For a binary classification problem (e.g., Positive vs. Negative), it is a 2x2 matrix showing the counts of correct and incorrect predictions across four categories:</p>\n",
    "  <p>\n",
    "      <ul>\n",
    "          <li><strong>True Positive (TP)</strong>: The model correctly predicts the positive class (e.g., correctly identifies a spam email as spam).</li>\n",
    "          <li><strong>True Negative (TN)</strong>: The model correctly predicts the negative class (e.g., correctly identifies a non-spam email as non-spam).</li>\n",
    "          <li><strong>False Positive (FP)</strong>: The model incorrectly predicts the positive class (e.g., a non-spam email is wrongly classified as spam).</li>\n",
    "          <li><strong>False Negative (FN)</strong>: The model incorrectly predicts the negative class (e.g., a spam email is wrongly classified as non-spam).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Structure</strong>: The confusion matrix is typically represented as:</p>\n",
    "  <table class=\"table\">\n",
    "      <tr>\n",
    "          <th></th>\n",
    "          <th>Predicted Positive</th>\n",
    "          <th>Predicted Negative</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <th>Actual Positive</th>\n",
    "          <td>TP</td>\n",
    "          <td>FN</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <th>Actual Negative</th>\n",
    "          <td>FP</td>\n",
    "          <td>TN</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "  <p><strong>Intuitive Analogy</strong>: Imagine you’re sorting 100 emails into \"Spam\" (Positive) or \"Not Spam\" (Negative). The confusion matrix is like a report card showing:\n",
    "      <ul>\n",
    "          <li>How many spam emails you correctly flagged (TP).</li>\n",
    "          <li>How many non-spam emails you correctly let through (TN).</li>\n",
    "          <li>How many non-spam emails you accidentally flagged as spam (FP, annoying for users!).</li>\n",
    "          <li>How many spam emails you missed and let into the inbox (FN, risky!).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Example: Spam Detection</strong>: Suppose you have a dataset of 100 emails, with 40 actually being spam (Positive) and 60 non-spam (Negative). Your model makes the following predictions:\n",
    "      <ul>\n",
    "          <li>30 emails correctly classified as spam (TP = 30).</li>\n",
    "          <li>50 emails correctly classified as non-spam (TN = 50).</li>\n",
    "          <li>10 emails incorrectly classified as spam (FP = 10).</li>\n",
    "          <li>10 emails incorrectly classified as non-spam (FN = 10).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p>The confusion matrix is:</p>\n",
    "  <table class=\"table\">\n",
    "      <tr>\n",
    "          <th></th>\n",
    "          <th>Predicted Spam</th>\n",
    "          <th>Predicted Not Spam</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <th>Actual Spam</th>\n",
    "          <td>30 (TP)</td>\n",
    "          <td>10 (FN)</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "          <th>Actual Not Spam</th>\n",
    "          <td>10 (FP)</td>\n",
    "          <td>50 (TN)</td>\n",
    "      </tr>\n",
    "  </table>\n",
    "  <p><strong>Why It’s Useful</strong>:\n",
    "      <ul>\n",
    "          <li>Shows the full picture of model performance, not just overall accuracy.</li>\n",
    "          <li>Helps identify specific errors (e.g., too many false positives).</li>\n",
    "          <li>Critical for imbalanced datasets (e.g., when spam emails are rare).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Multi-Class Extension</strong>: For K classes, the confusion matrix is a KxK table, where element (i,j) represents the number of samples of class i predicted as class j. The diagonal (i,i) shows correct predictions (true positives for each class).</p>\n",
    "  <img src=\"./assets/confutsion matrix.avif\" class=\"image-placeholder\">\n",
    "\n",
    "  <h2>Key Evaluation Metrics</h2>\n",
    "  <p>Evaluation metrics derived from the confusion matrix quantify different aspects of a classification model’s performance. Below are the key metrics with their formulas and interpretations, using the confusion matrix terms (TP, TN, FP, FN).</p>\n",
    "\n",
    "  <h3>Accuracy</h3>\n",
    "  <p>Accuracy measures the proportion of correct predictions (both positive and negative) out of all predictions:</p>\n",
    "  <div class=\"math-block\">\n",
    "      Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  </div>\n",
    "  <p><strong>Interpretation</strong>: Tells you how often the model is correct overall. In the spam example:\n",
    "      <ul>\n",
    "          <li>Accuracy = (30 + 50) / (30 + 50 + 10 + 10) = 80/100 = 0.8 (80% correct).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Limitation</strong>: Misleading for imbalanced datasets (e.g., if 95% of emails are non-spam, a model predicting everything as non-spam could still have high accuracy).</p>\n",
    "\n",
    "  <h3>Precision</h3>\n",
    "  <p>Precision measures the proportion of predicted positive cases that are actually positive:</p>\n",
    "  <div class=\"math-block\">\n",
    "      Precision = TP / (TP + FP)\n",
    "  </div>\n",
    "  <p><strong>Interpretation</strong>: Answers, “When the model predicts spam, how often is it actually spam?” In the spam example:\n",
    "      <ul>\n",
    "          <li>Precision = 30 / (30 + 10) = 30/40 = 0.75 (75% of predicted spam emails are truly spam).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Use Case</strong>: High precision is critical when false positives are costly (e.g., flagging a legitimate email as spam annoys users).</p>\n",
    "\n",
    "  <h3>Recall (Sensitivity or True Positive Rate)</h3>\n",
    "  <p>Recall measures the proportion of actual positive cases that are correctly identified:</p>\n",
    "  <div class=\"math-block\">\n",
    "      Recall = TP / (TP + FN)\n",
    "  </div>\n",
    "  <p><strong>Interpretation</strong>: Answers, “How many spam emails did the model catch out of all actual spam emails?” In the spam example:\n",
    "      <ul>\n",
    "          <li>Recall = 30 / (30 + 10) = 30/40 = 0.75 (75% of actual spam emails were caught).</li>\n",
    "      </ul>\n",
    "  </p>\n",
    "  <p><strong>Use Case</strong>: High recall is critical when false negatives are costly (e.g., missing a spam email could let malware into the system).</p>\n",
    "</section>\n",
    "\n",
    "<section id=\"section14\">\n",
    "<h2>Introduction to Unsupervised Learning</h2>\n",
    "<p>Unsupervised learning involves training models on data without labeled outputs, aiming to discover hidden patterns or structures. Unlike supervised learning, where the model learns from input-output pairs (x, y), unsupervised learning works with only inputs (x) and seeks to group similar data points or reduce data complexity.</p>\n",
    "<p><strong>Main Tasks</strong>:\n",
    "    <ul>\n",
    "        <li><strong>Clustering</strong>: Grouping similar data points into clusters based on their features (e.g., grouping customers by purchasing behavior).</li>\n",
    "        <li><strong>Dimensionality Reduction</strong>: Reducing the number of features while preserving important information (e.g., compressing high-dimensional data for visualization).</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p><strong>Applications</strong>:\n",
    "    <ul>\n",
    "        <li>Market segmentation (clustering customers).</li>\n",
    "        <li>Data compression (reducing image dimensions).</li>\n",
    "        <li>Anomaly detection (identifying outliers).</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<img src=\"./assets/unsuper.png\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section15\">\n",
    "<h2>Clustering</h2>\n",
    "<p>Clustering algorithms group data points into clusters based on similarity, where similarity is often measured by a distance metric (e.g., Euclidean distance). Each cluster contains data points that are more similar to each other than to those in other clusters.</p>\n",
    "\n",
    "<h3>K-Means Clustering</h3>\n",
    "<p>K-Means is a popular clustering algorithm that partitions data into K clusters by minimizing the variance within each cluster. It assumes clusters are spherical and of similar size.</p>\n",
    "<p><strong>Objective</strong>: Minimize the within-cluster sum of squares (WCSS):</p>\n",
    "<div class=\"math-block\">\n",
    "J = Σᵢ₌₁ⁿ Σₖ₌₁ᴷ r_{ik} ||xᵢ - μₖ||²\n",
    "</div>\n",
    "<p>where:\n",
    "<ul>\n",
    "    <li>xᵢ: Data point i.</li>\n",
    "    <li>μₖ: Centroid of cluster k.</li>\n",
    "    <li>r_{ik}: Indicator (1 if xᵢ is in cluster k, 0 otherwise).</li>\n",
    "    <li>||xᵢ - μₖ||²: Squared Euclidean distance.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p><strong>Algorithm</strong>:\n",
    "<ol>\n",
    "    <li>Choose K (number of clusters).</li>\n",
    "    <li>Initialize K centroids randomly (e.g., by selecting K data points).</li>\n",
    "    <li>Assign each data point to the nearest centroid based on Euclidean distance.</li>\n",
    "    <li>Update centroids by computing the mean of all points in each cluster.</li>\n",
    "    <li>Repeat steps 3-4 until centroids stabilize or a maximum number of iterations is reached.</li>\n",
    "</ol>\n",
    "</p>\n",
    "<p><strong>Example</strong>: Imagine a store wants to group 100 customers based on their spending and visit frequency. K-Means might identify 3 clusters: high spenders, frequent visitors with low spending, and occasional shoppers.</p>\n",
    "<p><strong>Challenges</strong>:\n",
    "<ul>\n",
    "    <li>Requires specifying K in advance.</li>\n",
    "    <li>Sensitive to initial centroid placement (solved by K-Means++ initialization).</li>\n",
    "    <li>Assumes spherical clusters, struggles with irregular shapes.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<img src=\"./assets/kmeans.gif\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<section id=\"section16\">\n",
    "<h2>Dimensionality Reduction</h2>\n",
    "<p>Dimensionality reduction techniques reduce the number of features in a dataset while preserving as much information as possible. This is useful for visualization, noise reduction, and improving computational efficiency.</p>\n",
    "<h3>Principal Component Analysis (PCA)</h3>\n",
    "<p>PCA is a linear dimensionality reduction technique that projects data onto a lower-dimensional space by maximizing variance. It finds orthogonal axes (principal components) that capture the most variability in the data.</p>\n",
    "<p><strong>Objective</strong>: Maximize the variance of the projected data. For a dataset X (n samples, m features), PCA finds k principal components (k < m) by solving:</p>\n",
    "<div class=\"math-block\">\n",
    "max Σᵢ₌₁ⁿ ||Pᵀ xᵢ||² subject to PᵀP = I\n",
    "</div>\n",
    "<p>where:\n",
    "<ul>\n",
    "    <li>P: Matrix of principal components (orthogonal vectors).</li>\n",
    "    <li>xᵢ: Data point i (centered, i.e., mean-subtracted).</li>\n",
    "    <li>I: Identity matrix (ensures orthogonality).</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p><strong>Algorithm</strong>:\n",
    "<ol>\n",
    "    <li>Standardize the data (mean = 0, variance = 1).</li>\n",
    "    <li>Compute the covariance matrix: Σ = (1/n) XᵀX.</li>\n",
    "    <li>Perform eigenvalue decomposition on Σ to get eigenvectors (principal components) and eigenvalues (variance explained).</li>\n",
    "    <li>Select the top k eigenvectors corresponding to the largest eigenvalues.</li>\n",
    "    <li>Project the data onto these k components: X' = XP.</li>\n",
    "</ol>\n",
    "</p>\n",
    "<p><strong>Example</strong>: In image processing, PCA can reduce a 1000-pixel image (1000 features) to 50 principal components, preserving most visual information for visualization or compression.</p>\n",
    "<p><strong>Variance Explained</strong>: The proportion of variance explained by the k-th component is:</p>\n",
    "<div class=\"math-block\">\n",
    "Variance_k = λ_k / Σᵢ₌₁ᵐ λᵢ\n",
    "</div>\n",
    "<p>where λ_k is the eigenvalue of the k-th component.</p>\n",
    "<p><strong>Advantages</strong>:\n",
    "<ul>\n",
    "    <li>Reduces dimensionality while preserving linear structure.</li>\n",
    "    <li>Useful for noise reduction and visualization.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<p><strong>Challenges</strong>:\n",
    "<ul>\n",
    "    <li>Assumes linear relationships.</li>\n",
    "    <li>Requires standardized data.</li>\n",
    "    <li>Interpretation of components can be difficult.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<img src=\"./assets/pca.gif\" class=\"image-placeholder\">\n",
    "<img src=\"./assets/pca2.webp\" class=\"image-placeholder\">\n",
    "</section>\n",
    "\n",
    "<p><a href=\"#section1\" class=\"button\">Back to Top</a></p>\n",
    "\n",
    "<div class=\"created-by\">\n",
    "Created by <a href=\"https://wikm.ir\" target=\"_blank\">wikm.ir</a> with <span>❤️</span>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "</body>\n",
    "</html>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
